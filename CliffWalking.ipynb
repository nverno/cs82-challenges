{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge Assignment\n",
    "## Cliff Walking with Reinforcement Learning\n",
    "\n",
    "## CSCI E-82A\n",
    "\n",
    ">**Make sure** you include your name along with the name of your team and team members in the notebook you submit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your name and team name here:** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this challenge you will apply Monte Carlo reinforcement learning algorithms to a classic problem in reinforcement learning, known as the **cliff walking problem**. The cliff walking problem is a type of game. The goal is for the agent to find the highest reward (lowest cost) path from a starting state to the goal.   \n",
    "\n",
    "There are a number of versions of the cliff walking problems which have been used as research benchmarks over the years. You can find a short discussion of the cliff walking problem on page 132 of Sutton and Barto, second edition.    \n",
    "\n",
    "In the general cliff walking problem the agent starts in one corner of the state-space and must travel to goal, or terminal state, in another corner of the state-space. Between the starting state and goal state there is an area with a **cliff**. If the agent falls off a cliff it is sent back to the starting state. A schematic diagram of the state-space is shown in the diagram below.      \n",
    "\n",
    "<img src=\"CliffWalkingDiagram.JPG\" alt=\"Drawing\" style=\"width:500px; height:400px\"/>\n",
    "<center> State-space of cliff-walking problem </center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Description\n",
    "\n",
    "The agent must learn a policy to navigate from the starting state to the terminal state. The properties this problem are as follows:\n",
    "\n",
    "1. The state-space has two **continuous variables**, x and y.\n",
    "2. The starting state is at $x = 0.0$, $y = 0.0$. \n",
    "3. The terminal state has two segments:\n",
    "  - At $y = 0.0$ is in the range $9.0 \\le x \\le 10.0$. \n",
    "  - At $x = 10.0$ is in the range $0.0 \\le y \\le 1.0$.  \n",
    "4. The cliff zone is bounded by:\n",
    "  - $0.0 \\le y \\le 1.0$ and \n",
    "  - $1.0 \\le x \\le 9.0$. \n",
    "5. An agent entering the cliff zone is returned to the starting state.\n",
    "6. The agent moves 1.0 units per time step. \n",
    "7. The 8 possible **discrete actions** are moves in the following directions:  \n",
    "  - +x, \n",
    "  - +x, +y,\n",
    "  - +y\n",
    "  - -x, +y,\n",
    "  - -y,\n",
    "  - -x, -y,\n",
    "  - -y, and\n",
    "  - +x, -y. \n",
    "8. The rewards are:\n",
    "  - -1 for a time step in the state-space,\n",
    "  - -10 for colliding with an edge (barrier) of the state-space,\n",
    "  - -100 for falling off the cliff and returning to the starting state, and \n",
    "  - +1000 for reaching the terminal or goal state. \n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "In this challenge you and your team will do the following. Include commentary on each component of your algorithms. Make sure you answer the questions.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Simulator   \n",
    "\n",
    "Your reinforcement learning agent cannot contain any information about the environment other that the starting state and the possible actions. Therefore, you must create an environment simulator, with the following input and output:\n",
    "- Input: Arguments of state, the $(x,y)$ tuple, and discrete action\n",
    "- Output: the new state (s'), reward, and if the new state meets the terminal or goal criteria.\n",
    "\n",
    "Make sure you test your simulator functions carefully. The test cases must include, steps with each of the actions, falling off the cliff from each edge, hitting the barriers, and reaching the goal (terminal) edges. Errors in the simulator will make the rest of this challenge difficult.   \n",
    "\n",
    "> **Note**: For this problem, coordinate state is represented by a tuple of continuous variables. Make sure that you maintain coordinate state as continuous variables for this problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import cos\n",
    "import numpy as np\n",
    "import numpy.random as nr\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def sim_environment(x, y, action):\n",
    "    fell = False\n",
    "    hit_wall = False\n",
    "    # calculate new location\n",
    "    x_prime = x\n",
    "    y_prime = y\n",
    "    if (action[0] == 1):\n",
    "        if (action[1] == 1):\n",
    "            x_prime = x + .707\n",
    "            y_prime = y + .707\n",
    "        elif(action[1] == -1):\n",
    "            x_prime = x + .707\n",
    "            y_prime = y - .707\n",
    "        else:\n",
    "            x_prime = x + 1\n",
    "    elif (action[0] == -1):\n",
    "        if (action[1] == 1):\n",
    "            x_prime = x - .707\n",
    "            y_prime = y + .707\n",
    "        elif(action[1] == -1):\n",
    "            x_prime = x - .707\n",
    "            y_prime = y - .707\n",
    "        else:\n",
    "            x_prime = x - 1\n",
    "    else:\n",
    "        if (action[1] == 1):\n",
    "            y_prime = y + 1\n",
    "        elif (action[1] == -1):\n",
    "            y_prime = y - 1\n",
    "             \n",
    "    #ensure new location is in bounds\n",
    "    if (x_prime > 10.0):\n",
    "        x_prime = 0.0\n",
    "        y_prime = 0.0\n",
    "        hit_wall = True\n",
    "    elif (x_prime < 0.0):\n",
    "        x_prime = 0.0\n",
    "        y_prime = 0.0\n",
    "        hit_wall = True\n",
    "    if (y_prime > 10.0):\n",
    "        x_prime = 0.0\n",
    "        y_prime = 0.0\n",
    "        hit_wall = True\n",
    "    elif (y_prime < 0.0):\n",
    "        x_prime = 0.0\n",
    "        y_prime = 0.0  \n",
    "        hit_wall = True\n",
    "             \n",
    "    #ensure new location is not off cliff\n",
    "    if (x_prime >= 1.0 and x_prime <= 9.0):\n",
    "        if (y_prime >= 0.0 and y_prime <= 1.0):\n",
    "             x_prime = 0.0\n",
    "             y_prime = 0.0\n",
    "             fell = True\n",
    "    \n",
    "    ## At the terminal state or not and set reward\n",
    "    if (in_terminal(x_prime, y_prime)):\n",
    "        done = True\n",
    "        reward = 1000\n",
    "    elif (fell):\n",
    "        done = False\n",
    "        reward = -100\n",
    "    elif (hit_wall):\n",
    "        done = False\n",
    "        reward = -10\n",
    "        x_prime = x\n",
    "        y_prime = y\n",
    "    else:\n",
    "        done = False\n",
    "        reward = -1.0\n",
    "    # output new state (s'), reward, and if the new state meets the terminal or goal criteria\n",
    "    return(x_prime, y_prime, done, reward)\n",
    "\n",
    "def in_terminal(x, y):\n",
    "    if (y == 0.0 and x >= 9.0):\n",
    "        return True\n",
    "    elif (x == 10.0 and y <= 1.0):\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Approximation\n",
    "\n",
    "The state-space of the cliff walking problem is continuous. Therefor, you will need to use a **grid approximation** to construct a policy. The policy is specified as the probability of action for each grid cell. For this problem, use a 10x10 grid. \n",
    "\n",
    "> **Note:** While the policy uses a grid approximation, state should be represented as continuous variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def state(x, x_lims = (0.0,10.0), n_tiles = 10):\n",
    "    \"\"\"Function to compute tile state given positon\"\"\"\n",
    "    state = int((x - x_lims[0])/(x_lims[1] - x_lims[0]) * float(n_tiles))\n",
    "    if(state > n_tiles - 1): state = n_tiles - 1\n",
    "    return(state)\n",
    "\n",
    "def get_grid_state(x, y):\n",
    "    return (state(x) + 10 * state(y))\n",
    "\n",
    "def get_index(num):\n",
    "    return num // 10, num % 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Policy\n",
    "\n",
    "Start with a uniform initial policy. A uniform policy has an equal probability of taking any of the 8 possible actions for each cell in the grid representation.     \n",
    "\n",
    "> **Note:** As has already been stated, the coordinate state representation for this problem is a tuple of coordinate values. However, policy, state-values and action-values are represented with a grid approximation. \n",
    "\n",
    "> **Hint:** You may wish to use a 3-dimensional numpy array to code the policy for this problem. With 8 possible actions, this approach will be easier to work with. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125],\n",
       "  [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125],\n",
       "  [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125],\n",
       "  [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125],\n",
       "  [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125],\n",
       "  [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125],\n",
       "  [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125],\n",
       "  [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125],\n",
       "  [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125],\n",
       "  [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125]],\n",
       " [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125],\n",
       "  [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125],\n",
       "  [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125],\n",
       "  [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125],\n",
       "  [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125],\n",
       "  [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125],\n",
       "  [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125],\n",
       "  [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125],\n",
       "  [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125],\n",
       "  [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125]],\n",
       " [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125],\n",
       "  [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125],\n",
       "  [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125],\n",
       "  [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125],\n",
       "  [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125],\n",
       "  [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125],\n",
       "  [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125],\n",
       "  [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125],\n",
       "  [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125],\n",
       "  [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125]],\n",
       " [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125],\n",
       "  [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125],\n",
       "  [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125],\n",
       "  [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125],\n",
       "  [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125],\n",
       "  [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125],\n",
       "  [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125],\n",
       "  [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125],\n",
       "  [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125],\n",
       "  [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125]],\n",
       " [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125],\n",
       "  [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125],\n",
       "  [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125],\n",
       "  [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125],\n",
       "  [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125],\n",
       "  [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125],\n",
       "  [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125],\n",
       "  [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125],\n",
       "  [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125],\n",
       "  [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125]],\n",
       " [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125],\n",
       "  [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125],\n",
       "  [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125],\n",
       "  [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125],\n",
       "  [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125],\n",
       "  [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125],\n",
       "  [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125],\n",
       "  [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125],\n",
       "  [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125],\n",
       "  [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125]],\n",
       " [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125],\n",
       "  [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125],\n",
       "  [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125],\n",
       "  [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125],\n",
       "  [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125],\n",
       "  [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125],\n",
       "  [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125],\n",
       "  [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125],\n",
       "  [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125],\n",
       "  [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125]],\n",
       " [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125],\n",
       "  [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125],\n",
       "  [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125],\n",
       "  [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125],\n",
       "  [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125],\n",
       "  [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125],\n",
       "  [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125],\n",
       "  [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125],\n",
       "  [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125],\n",
       "  [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125]],\n",
       " [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125],\n",
       "  [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125],\n",
       "  [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125],\n",
       "  [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125],\n",
       "  [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125],\n",
       "  [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125],\n",
       "  [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125],\n",
       "  [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125],\n",
       "  [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125],\n",
       "  [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125]],\n",
       " [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125],\n",
       "  [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125],\n",
       "  [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125],\n",
       "  [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125],\n",
       "  [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125],\n",
       "  [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125],\n",
       "  [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125],\n",
       "  [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125],\n",
       "  [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125],\n",
       "  [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125]]]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_policy = [[[.125 for i in range(8)] for j in range(10)] for k in range(10)]\n",
    "initial_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte Carlo State Value Estimation   \n",
    "\n",
    "For the initial uniform policy, compute the state values using the Monte Carlo RL algorithm:\n",
    "1. Compute and print the state values for each grid in the representation. Use at least 1,000 episodes. This will take some time to execute.      \n",
    "2. Plot the grid of state values, as an image (e.g. matplotlib [imshow](https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.pyplot.imshow.html)). \n",
    "3. Compute the Forbenious norm (Euclidean norm) of the state value array with [numpy.linalg.norm](https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.norm.html). You will use this figure as a basis to compare your improved policy. \n",
    "\n",
    "Study your plot to ensure your state values seem correct. Do these state values seem reasonable given the uniform policy and why? Make sure you pay attention to the state values of the cliff zone.    \n",
    "\n",
    "> **Hint:** Careful testing at each stage of your algorithm development will potentially save you considerable time. Test your function(s) to for a single episode to make sure your algorithm converges. Then test for say 10 episodes to ensure the state values update in a reasonable manner at each episode.    \n",
    "\n",
    "> **Note:** The Monte Carlo episodes can be executed in parallel for production systems. The Markov chain of each episode is statistically independent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running episode: 0\n",
      "running episode: 1\n",
      "running episode: 2\n",
      "running episode: 3\n",
      "running episode: 4\n",
      "running episode: 5\n",
      "running episode: 6\n",
      "running episode: 7\n",
      "running episode: 8\n",
      "running episode: 9\n",
      "running episode: 10\n",
      "running episode: 11\n",
      "running episode: 12\n",
      "running episode: 13\n",
      "running episode: 14\n",
      "running episode: 15\n",
      "running episode: 16\n",
      "running episode: 17\n",
      "running episode: 18\n",
      "running episode: 19\n",
      "running episode: 20\n",
      "running episode: 21\n",
      "running episode: 22\n",
      "running episode: 23\n",
      "running episode: 24\n",
      "running episode: 25\n",
      "running episode: 26\n",
      "running episode: 27\n",
      "running episode: 28\n",
      "running episode: 29\n",
      "running episode: 30\n",
      "running episode: 31\n",
      "running episode: 32\n",
      "running episode: 33\n",
      "running episode: 34\n",
      "running episode: 35\n",
      "running episode: 36\n",
      "running episode: 37\n",
      "running episode: 38\n",
      "running episode: 39\n",
      "running episode: 40\n",
      "running episode: 41\n",
      "running episode: 42\n",
      "running episode: 43\n",
      "running episode: 44\n",
      "running episode: 45\n",
      "running episode: 46\n",
      "running episode: 47\n",
      "running episode: 48\n",
      "running episode: 49\n",
      "running episode: 50\n",
      "running episode: 51\n",
      "running episode: 52\n",
      "running episode: 53\n",
      "running episode: 54\n",
      "running episode: 55\n",
      "running episode: 56\n",
      "running episode: 57\n",
      "running episode: 58\n",
      "running episode: 59\n",
      "running episode: 60\n",
      "running episode: 61\n",
      "running episode: 62\n",
      "running episode: 63\n",
      "running episode: 64\n",
      "running episode: 65\n",
      "running episode: 66\n",
      "running episode: 67\n",
      "running episode: 68\n",
      "running episode: 69\n",
      "running episode: 70\n",
      "running episode: 71\n",
      "running episode: 72\n",
      "running episode: 73\n",
      "running episode: 74\n",
      "running episode: 75\n",
      "running episode: 76\n",
      "running episode: 77\n",
      "running episode: 78\n",
      "running episode: 79\n",
      "running episode: 80\n",
      "running episode: 81\n",
      "running episode: 82\n",
      "running episode: 83\n",
      "running episode: 84\n",
      "running episode: 85\n",
      "running episode: 86\n",
      "running episode: 87\n",
      "running episode: 88\n",
      "running episode: 89\n",
      "running episode: 90\n",
      "running episode: 91\n",
      "running episode: 92\n",
      "running episode: 93\n",
      "running episode: 94\n",
      "running episode: 95\n",
      "running episode: 96\n",
      "running episode: 97\n",
      "running episode: 98\n",
      "running episode: 99\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-4.77713664,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        , -8.83950617],\n",
       "       [-4.47791657, -4.53923824, -4.72849639, -4.6952003 , -5.29805161,\n",
       "        -6.07311157, -6.10780526, -6.44702406, -6.74046558, -7.00787402],\n",
       "       [-4.35732469, -4.38271906, -4.39890639, -4.56516443, -4.85998071,\n",
       "        -4.99690402, -5.40571089, -6.50190114, -6.14905263, -6.29191216],\n",
       "       [-4.23597033, -4.3156543 , -4.30262984, -4.42740444, -4.68454026,\n",
       "        -4.90204448, -5.14237552, -5.47402597, -5.88632619, -6.09341826],\n",
       "       [-4.26169388, -4.26577653, -4.33107426, -4.44900828, -4.59582005,\n",
       "        -4.78993126, -5.04694948, -5.25754251, -5.35574837, -5.84591915],\n",
       "       [-4.40329903, -4.36065815, -4.42613548, -4.51669209, -4.46155248,\n",
       "        -4.59818433, -5.0056221 , -5.16038697, -5.34782609, -5.66779026],\n",
       "       [-4.6637869 , -4.52120498, -4.54435817, -4.51995817, -4.66727309,\n",
       "        -4.69436446, -4.67151857, -4.98723626, -5.05717397, -5.70756492],\n",
       "       [-4.74192254, -4.65557587, -4.72274315, -4.55400038, -4.70359341,\n",
       "        -4.73463137, -4.77056779, -4.85218254, -5.11482558, -5.45738742],\n",
       "       [-4.76029616, -4.60147524, -4.6928684 , -4.69450549, -4.75921635,\n",
       "        -4.67924086, -4.85885519, -5.04218022, -5.24123238, -5.97106691],\n",
       "       [-4.92735278, -4.71542212, -4.78847631, -4.73942801, -5.0120353 ,\n",
       "        -5.11991377, -5.06538049, -5.35493923, -5.78676799, -6.        ]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import matplotlib.pyplot as plt\n",
    "actions = [(0,1), (0,-1), (1,0), (1,1), (1,-1), (-1,0), (-1,1), (-1,-1)]\n",
    "    \n",
    "def take_action(x, y, policy):\n",
    "    '''Function takes action given state using the transition probabilities \n",
    "    of the policy'''\n",
    "    ## Find the action given the transistion probabilities defined by the policy.\n",
    "    x_grid = state(x)\n",
    "    y_grid = state(y)\n",
    "    action = actions[nr.choice(8, p = policy[x_grid][y_grid])]\n",
    "    x_prime, y_prime, is_terminal, reward = sim_environment(x,y, action)\n",
    "    return (action, x_prime, y_prime, is_terminal, reward)\n",
    "\n",
    "#print(initial_policy[0][0])\n",
    "#take_action(0,0,initial_policy)\n",
    "\n",
    "def MC_episode(policy, G, n_visits): \n",
    "    '''Function creates the Monte Carlo samples of one episode.\n",
    "    This function does most of the real work'''\n",
    "    ## For each episode we use a list to keep track of states we have visited.\n",
    "    ## Once we visit a state we need to accumulate values to get the returns\n",
    "    states_visited = np.zeros(100)\n",
    "    visited = []\n",
    "    \n",
    "    ## Find the starting state\n",
    "    x_current = 0.0\n",
    "    y_current = 0.0\n",
    "    current_state = get_grid_state(x_current, y_current)\n",
    "    terminal = False\n",
    "    g = 0.0\n",
    "    #i = 0\n",
    "    #plt.axis([0, 10, 0, 10])\n",
    "    #plt.scatter(x_current, y_current)\n",
    "    while(not terminal):\n",
    "        ## Find the next action and reward\n",
    "        action, x_prime, y_prime, terminal, reward = take_action(x_current, y_current, policy)\n",
    "        #print(\"from: \" + str((x_current, y_current, action)))\n",
    "        #print(\"to: \" + str((x_prime, y_prime, is_terminal, reward)))\n",
    "        ## Add the reward to the states visited if this is a first visit  \n",
    "        if(states_visited[current_state] == 0):\n",
    "            ## Mark that the current state has been visited \n",
    "            states_visited[current_state] = 1\n",
    "            visited.append(current_state)\n",
    "            ## Add the reward to states visited \n",
    "            for state in visited:\n",
    "                n_visits[state] = n_visits[state] + 1.0\n",
    "                G[state] = G[state] + (reward - G[state])/n_visits[state]\n",
    "        \n",
    "        ## Update the current state for next transition\n",
    "        current_state = get_grid_state(x_prime, y_prime) \n",
    "        x_current = x_prime\n",
    "        y_current = y_prime\n",
    "        #i = i + 1\n",
    "        #plt.scatter(x_current, y_current)\n",
    "    #plt.show()\n",
    "    return (G, n_visits) \n",
    "\n",
    "def MC_state_values(policy, n_episodes):\n",
    "    '''Function that evaluates the state value of \n",
    "    a policy using the Monte Carlo method.'''\n",
    "    ## Create list of states \n",
    "    n_states = 100\n",
    "    \n",
    "    ## An array to hold the accumulated returns as we visit states\n",
    "    G = np.zeros((n_states))\n",
    "    \n",
    "    ## An array to keep track of how many times we visit each state so we can \n",
    "    ## compute the mean\n",
    "    n_visits = np.zeros((n_states))\n",
    "    #print(n_visits)\n",
    "    \n",
    "    ## Iterate over the episodes\n",
    "    for i in range(n_episodes):\n",
    "        G, n_visits = MC_episode(policy, G, n_visits) # neighbors, i, n_states)\n",
    "        print (\"running episode: \" + str(i))\n",
    "    return(G) \n",
    "\n",
    "MC_state_values(initial_policy, 100).reshape((10,10))\n",
    "#actions[nr.choice(8, p = initial_policy[0][0]) + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANS:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte Carlo State Policy Improvement   \n",
    "\n",
    "Finally, you will perform Monte Carlo RL policy improvement:\n",
    "1. Starting with the uniform policy, compute action-values for each grid in the representation. Use at least 1,000 episodes.      \n",
    "2. Use these action values to find an improved policy.\n",
    "3. To evaluate your updated policy compute the state-values for this policy.  \n",
    "4. Plot the grid of state values for the improved policy, as an image. \n",
    "5. Compute the Forbenious norm (Euclidean norm) of the state value array. \n",
    "\n",
    "Compare the state value plot for the improved policy to the one for the initial uniform policy. Does the improved state values increase generally as distance to the terminal states decreases?  Is this what you expect and why?    \n",
    "\n",
    "Compare the norm of the state values with your improved policy to the norm for the uniform policy. Is the increase significant?  \n",
    "\n",
    "> **Hint:** Careful testing at each stage of your algorithm development will potentially save you considerable time. Test your function(s) to for a single episode to make sure your algorithm converges. Then test for say 10 episodes to ensure the state values update in a reasonable manner at each episode.   \n",
    "\n",
    "> **Note:** You could continue to improve policy using the general policy improvement algorithm (GPI). In the interest of time, you are not required to do so here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running episode #: 0\n",
      "running episode #: 1\n",
      "running episode #: 2\n",
      "running episode #: 3\n",
      "running episode #: 4\n",
      "running episode #: 5\n",
      "running episode #: 6\n",
      "running episode #: 7\n",
      "running episode #: 8\n",
      "running episode #: 9\n",
      "running episode #: 10\n",
      "running episode #: 11\n",
      "running episode #: 12\n",
      "running episode #: 13\n",
      "running episode #: 14\n",
      "running episode #: 15\n",
      "running episode #: 16\n",
      "running episode #: 17\n",
      "running episode #: 18\n",
      "running episode #: 19\n",
      "running episode #: 20\n",
      "running episode #: 21\n",
      "running episode #: 22\n",
      "running episode #: 23\n",
      "running episode #: 24\n",
      "running episode #: 25\n",
      "running episode #: 26\n",
      "running episode #: 27\n",
      "running episode #: 28\n",
      "running episode #: 29\n",
      "running episode #: 30\n",
      "running episode #: 31\n",
      "running episode #: 32\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import pandas as pd\n",
    "def get_action_index(action):\n",
    "    if (action == (0,1)): return 0\n",
    "    elif (action == (0,-1)): return 1\n",
    "    elif (action == (1,0)): return 2\n",
    "    elif (action == (1,1)): return 3\n",
    "    elif (action == (1,-1)): return 4\n",
    "    elif (action == (-1,0)): return 5\n",
    "    elif (action == (-1,1)): return 6\n",
    "    elif (action == (-1,-1)): return 7\n",
    "    \n",
    "def print_Q(Q):\n",
    "    Q = pd.DataFrame(Q, columns = ['N', 'S', 'E', 'NE', 'SE', 'W', 'NW', 'SW'])\n",
    "    print(Q)\n",
    "    \n",
    "def MC_action_value_episode(policy, Q, n_visits, inital_state, n_states, n_actions, action_index = {'u':0, 'd':1, 'l':2, 'r':3}):\n",
    "    '''Function creates the Monte Carlo samples of action values for one episode.\n",
    "    This function does most of the real work'''\n",
    "    ## For each episode we use a list to keep track of states we have visited.\n",
    "    ## Once we visit a state we need to accumulate values to get the returns\n",
    "    state_actions_visited = np.zeros((n_states, n_actions))\n",
    "    x_current = 0.0\n",
    "    y_current = 0.0\n",
    "    current_state = get_grid_state(x_current, y_current)\n",
    "    terminal = False  \n",
    "    while(not terminal):\n",
    "        ## Find the next action and reward\n",
    "        #action, s_prime, reward, terminal = take_action(current_state, policy)\n",
    "        action, x_prime, y_prime, terminal, reward = take_action(x_current, y_current, policy)\n",
    "        action_idx = get_action_index(action)         \n",
    "        \n",
    "        ## Check if this state-action has been visited.\n",
    "        if(state_actions_visited[current_state, action_idx] != 1.0):\n",
    "            ## Mark that the current state-action has been visited \n",
    "            state_actions_visited[current_state, action_idx] = 1.0  \n",
    "            ## This is first vist MS, so must loop over all state-action pairs and \n",
    "            ## add the reward and increment the count for the ones visited.\n",
    "            for s,a in list(itertools.product(range(n_states), range(n_actions))):\n",
    "                ## Add reward to if these has been a visit to the state\n",
    "                if(state_actions_visited[s,a] == 1.0):\n",
    "                    n_visits[s,a] = n_visits[s,a] + 1.0\n",
    "                    Q[s,a] = Q[s,a] + (reward - Q[s,a])/n_visits[s,a]    \n",
    "        ## Update the current state for next transition\n",
    "        current_state = get_grid_state(x_prime, y_prime)\n",
    "        x_current = x_prime\n",
    "        y_current = y_prime\n",
    "    return (Q, n_visits) \n",
    "\n",
    "def MC_action_values(policy, Q, n_episodes, inital_state):\n",
    "    '''Function evaluates the action-values given a policy for the specified number of episodes and \n",
    "    initial state'''\n",
    "    n_states = 100\n",
    "    n_actions = 8\n",
    "    ## Array to count visits to action-value pairs\n",
    "    n_visits = np.zeros((n_states, n_actions))\n",
    "    ## Dictionary to hold neighbor states\n",
    "    neighbors = {}\n",
    "    \n",
    "    ## Loop over number of episodes\n",
    "    for i in range(n_episodes):\n",
    "        ## One episode of MC\n",
    "        print (\"running episode #: \" + str(i))\n",
    "        Q, n_visits = MC_action_value_episode(policy, Q, n_visits, initial_state, n_states, n_actions)\n",
    "    return(Q)\n",
    "\n",
    "n_episodes = 100\n",
    "initial_state = 0\n",
    "Q = np.zeros((100, 8))\n",
    "Q = MC_action_values(initial_policy, Q, n_episodes, initial_state)\n",
    "print_Q(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Q' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-912cf4a4856a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mupdate_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_policy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'Q' is not defined"
     ]
    }
   ],
   "source": [
    "def update_policy(policy, Q, epsilon, action_index = {'u':0, 'd':1, 'l':2, 'r':3}):\n",
    "    '''Updates the policy based on estiamtes of Q using \n",
    "    an epslion greedy algorithm. The action with the highest\n",
    "    action value is used.'''\n",
    "    \n",
    "    ## Find the keys for the actions in the policy\n",
    "    keys = actions\n",
    "    \n",
    "    ## Iterate over the states and find the maximm action value.\n",
    "    for state in range(len(policy)):\n",
    "        ## First find the index of the max Q values  \n",
    "        q = Q[state,:]\n",
    "        max_action_index = np.where(q == max(q))[0]\n",
    "\n",
    "        ## Find the probabilities for the transitions\n",
    "        n_transitions = float(len(q))\n",
    "        n_max_transitions = float(len(max_action_index))\n",
    "        p_max_transitions = (1.0 - epsilon *(n_transitions - n_max_transitions))/(n_max_transitions)\n",
    "  \n",
    "        ## Now assign the probabilities to the policy as epsilon greedy.\n",
    "        for key in keys:\n",
    "            if(action_index[key] in max_action_index): policy[state][key] = p_max_transitions\n",
    "            else: policy[state][key] = epsilon\n",
    "    return(policy)                \n",
    "\n",
    "update_policy(initial_policy, Q, 0.1)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANS:\n",
    "\n",
    "ANS:\n",
    "\n",
    "ANS:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution\n",
    "\n",
    "Create cells below for your solution to the stated problem. Be sure to include some Markdown text and code comments to explain each component of your algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
